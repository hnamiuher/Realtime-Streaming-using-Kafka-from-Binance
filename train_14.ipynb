{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f698a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training models for: BNBUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoain\\AppData\\Local\\Temp\\ipykernel_8020\\2671604090.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No scaling needed for BNBUSDT (median close = 597.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Training models for: BTCUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoain\\AppData\\Local\\Temp\\ipykernel_8020\\2671604090.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… No scaling needed for BTCUSDT (median close = 78999.61)\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary modules after kernel reset\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, window, first, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Re-define the core functions due to kernel reset\n",
    "def connect_to_postgres():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"airflow\",\n",
    "            user=\"airflow\",\n",
    "            password=\"airflow\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(f\"âŒ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_postgres(symbol):\n",
    "    conn = connect_to_postgres()\n",
    "    if conn:\n",
    "        query = f\"SELECT * FROM crypto_data WHERE symbol = '{symbol}';\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def prepare_timeseries_data(spark_df: DataFrame,\n",
    "                             start: datetime.datetime = datetime.datetime(2023, 1, 1, 0, 0, 0),\n",
    "                             end: datetime.datetime = datetime.datetime(2025, 10, 1, 0, 0, 0),\n",
    "                             window_duration: str = \"3 hours\",\n",
    "                             lag_days: int = 14) -> DataFrame:   # âœ… Äá»•i tÃªn biáº¿n táº¡i Ä‘Ã¢y\n",
    "\n",
    "    # BÆ°á»›c 1: Lá»c theo khoáº£ng thá»i gian\n",
    "    spark_df = spark_df.filter((col(\"time\") >= start) & (col(\"time\") <= end))\n",
    "    \n",
    "    # BÆ°á»›c 2: Gom nhÃ³m theo cá»­a sá»• thá»i gian\n",
    "    spark_df = spark_df.groupBy(\n",
    "        window(col(\"time\"), window_duration)\n",
    "    ).agg(\n",
    "        *[first(col_name).alias(col_name) for col_name in spark_df.columns if col_name != \"time\"]\n",
    "    )\n",
    "\n",
    "    # BÆ°á»›c 3: ThÃªm cá»™t thá»i gian Ä‘áº¡i diá»‡n nhÃ³m\n",
    "    spark_df = spark_df.withColumn(\"time_group\", col(\"window.start\")).drop(\"window\")\n",
    "\n",
    "    # BÆ°á»›c 4: Táº¡o Ä‘áº·c trÆ°ng lá»‹ch sá»­\n",
    "    window_spec = Window.orderBy(\"time_group\")\n",
    "    for i in range(1, lag_days + 1):    # âœ… DÃ¹ng láº¡i biáº¿n má»›i táº¡i Ä‘Ã¢y\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "            spark_df = spark_df.withColumn(f\"{col_name}_b_{i}\", lag(col_name, i).over(window_spec))\n",
    "\n",
    "    # BÆ°á»›c 5: Loáº¡i bá» cÃ¡c hÃ ng thiáº¿u dá»¯ liá»‡u lá»‹ch sá»­\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # BÆ°á»›c 6: Táº¡o nhÃ£n NEXT_CLOSE\n",
    "    spark_df = spark_df.withColumn(\"NEXT_CLOSE\", lead(\"close\", 1).over(window_spec))\n",
    "\n",
    "    # BÆ°á»›c 7: Loáº¡i bá» hÃ ng cuá»‘i cÃ¹ng khÃ´ng cÃ³ nhÃ£n\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # BÆ°á»›c 8: Loáº¡i bá» cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t\n",
    "    spark_df = spark_df.drop(\"symbol\", \"time\", \"time_group\")\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "def split_data(spark_df, prediction_days=750):\n",
    "    total_rows = spark_df.count()\n",
    "    train_rows = total_rows - prediction_days\n",
    "    train_df = spark_df.limit(train_rows)\n",
    "    test_df = spark_df.subtract(train_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def transform_to_labeledpoint(df: DataFrame):\n",
    "    return df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "\n",
    "def train_random_forest_regressor(train_rdd, num_trees=1000, max_depth=7, impurity=\"variance\", max_bins=300, seed=13579):\n",
    "    model = RandomForest.trainRegressor(\n",
    "        data=train_rdd,\n",
    "        categoricalFeaturesInfo={},\n",
    "        numTrees=num_trees,\n",
    "        featureSubsetStrategy=\"auto\",\n",
    "        impurity=impurity,\n",
    "        maxDepth=max_depth,\n",
    "        maxBins=max_bins,\n",
    "        seed=seed\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_linear_regression(train_df, test_df):\n",
    "    feature_cols = train_df.columns[:-1]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_df).select(\"features\", \"NEXT_CLOSE\")\n",
    "    test_data = assembler.transform(test_df).select(\"features\", \"NEXT_CLOSE\")\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"NEXT_CLOSE\")\n",
    "    model = lr.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    evaluator = RegressionEvaluator(labelCol=\"NEXT_CLOSE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    r2 = RegressionEvaluator(labelCol=\"NEXT_CLOSE\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions)\n",
    "    return rmse, r2\n",
    "\n",
    "def train_xgboost(train_df, test_df):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"]\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"]\n",
    "    model = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, objective=\"reg:squarederror\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds) ** 0.5  # manually take square root\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    return rmse, r2\n",
    "\n",
    "def evaluate_random_forest_model(model, test_rdd):\n",
    "    preds = model.predict(test_rdd.map(lambda x: x.features))\n",
    "    pred_label_rdd = preds.zip(test_rdd.map(lambda x: x.label))\n",
    "    metrics = RegressionMetrics(pred_label_rdd)\n",
    "    return metrics.rootMeanSquaredError, metrics.r2\n",
    "\n",
    "# Proceed to execute the training for 10 crypto symbols\n",
    "spark = SparkSession.builder.appName(\"CryptoPredict\").getOrCreate()\n",
    "\n",
    "symbols = [\"BNBUSDT\", \"BTCUSDT\", \"ETHUSDT\", \"XRPUSDT\", \"SOLUSDT\",\n",
    "           \"LTCUSDT\", \"ETCUSDT\", \"PEPEUSDT\", \"DOGEUSDT\", \"ADAUSDT\"]\n",
    "\n",
    "def train_all_models_for_one_symbol(symbol: str):\n",
    "    print(f\"\\nğŸš€ Training models for: {symbol}\")\n",
    "    df = load_data_from_postgres(symbol)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"âš ï¸ Skipping {symbol} due to empty data.\")\n",
    "        return None\n",
    "\n",
    "    # ğŸ’¡ Auto-scale small value coins\n",
    "    price_median = df[\"close\"].median()\n",
    "    scale_factor = 1.0\n",
    "    if price_median < 0.01:  # tuá»³ chá»‰nh ngÆ°á»¡ng nÃ y\n",
    "        scale_factor = 1e6\n",
    "        print(f\"ğŸ”§ Scaling {symbol} values by {scale_factor} due to small price.\")\n",
    "\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "            df[col_name] = df[col_name] * scale_factor\n",
    "        df[\"volume\"] = df[\"volume\"]  # khÃ´ng scale volume\n",
    "    else:\n",
    "        print(f\"âœ… No scaling needed for {symbol} (median close = {price_median})\")\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    processed_df = prepare_timeseries_data(spark_df)\n",
    "    train_df, test_df = split_data(processed_df)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        rmse, r2 = train_linear_regression(train_df, test_df)\n",
    "        results[\"linear_regression\"] = {\"rmse\": rmse, \"r2\": r2}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Linear Regression failed for {symbol}: {e}\")\n",
    "        results[\"linear_regression\"] = {\"rmse\": None, \"r2\": None}\n",
    "\n",
    "    try:\n",
    "        rmse, r2 = train_xgboost(train_df, test_df)\n",
    "        results[\"xgboost\"] = {\"rmse\": rmse, \"r2\": r2}\n",
    "        if r2 < 0:\n",
    "            print(f\"âš ï¸ XGBoost RÂ² < 0 for {symbol} â€” model might be worse than baseline.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ XGBoost failed for {symbol}: {e}\")\n",
    "        results[\"xgboost\"] = {\"rmse\": None, \"r2\": None}\n",
    "\n",
    "    try:\n",
    "        train_rdd = transform_to_labeledpoint(train_df)\n",
    "        test_rdd = transform_to_labeledpoint(test_df)\n",
    "        model = train_random_forest_regressor(train_rdd)\n",
    "        rmse, r2 = evaluate_random_forest_model(model, test_rdd)\n",
    "        results[\"random_forest\"] = {\"rmse\": rmse, \"r2\": r2}\n",
    "        if r2 < 0:\n",
    "            print(f\"âš ï¸ Random Forest RÂ² < 0 for {symbol}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Random Forest failed for {symbol}: {e}\")\n",
    "        results[\"random_forest\"] = {\"rmse\": None, \"r2\": None}\n",
    "\n",
    "    return {\"symbol\": symbol, \"results\": results}\n",
    "\n",
    "\n",
    "# Run for all symbols and collect results\n",
    "all_results = []\n",
    "for sym in symbols:\n",
    "    result = train_all_models_for_one_symbol(sym)\n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "# Format results into a DataFrame\n",
    "\n",
    "records = []\n",
    "for result in all_results:\n",
    "    symbol = result[\"symbol\"]\n",
    "    for model, metrics in result[\"results\"].items():\n",
    "        records.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"model\": model,\n",
    "            \"rmse\": metrics[\"rmse\"],\n",
    "            \"r2\": metrics[\"r2\"]\n",
    "        })\n",
    "        print(f\"Model: {model}, Symbol: {symbol}, RMSE: {metrics['rmse']}, R2: {metrics['r2']}\")\n",
    "\n",
    "pd.DataFrame(records)\n",
    "# Format results into a DataFrame\n",
    "records = []\n",
    "for result in all_results:\n",
    "    symbol = result[\"symbol\"]\n",
    "    for model, metrics in result[\"results\"].items():\n",
    "        records.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"model\": model,\n",
    "            \"rmse\": metrics[\"rmse\"],\n",
    "            \"r2\": metrics[\"r2\"]\n",
    "        })\n",
    "\n",
    "pd.DataFrame(records)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
