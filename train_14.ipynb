{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f698a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training models for: BNBUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoain\\AppData\\Local\\Temp\\ipykernel_8020\\2671604090.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No scaling needed for BNBUSDT (median close = 597.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training models for: BTCUSDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoain\\AppData\\Local\\Temp\\ipykernel_8020\\2671604090.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No scaling needed for BTCUSDT (median close = 78999.61)\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary modules after kernel reset\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, window, first, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Re-define the core functions due to kernel reset\n",
    "def connect_to_postgres():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"airflow\",\n",
    "            user=\"airflow\",\n",
    "            password=\"airflow\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_data_from_postgres(symbol):\n",
    "    conn = connect_to_postgres()\n",
    "    if conn:\n",
    "        query = f\"SELECT * FROM crypto_data WHERE symbol = '{symbol}';\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "def prepare_timeseries_data(spark_df: DataFrame,\n",
    "                             start: datetime.datetime = datetime.datetime(2023, 1, 1, 0, 0, 0),\n",
    "                             end: datetime.datetime = datetime.datetime(2025, 10, 1, 0, 0, 0),\n",
    "                             window_duration: str = \"3 hours\",\n",
    "                             lag_days: int = 14) -> DataFrame:   # ✅ Đổi tên biến tại đây\n",
    "\n",
    "    # Bước 1: Lọc theo khoảng thời gian\n",
    "    spark_df = spark_df.filter((col(\"time\") >= start) & (col(\"time\") <= end))\n",
    "    \n",
    "    # Bước 2: Gom nhóm theo cửa sổ thời gian\n",
    "    spark_df = spark_df.groupBy(\n",
    "        window(col(\"time\"), window_duration)\n",
    "    ).agg(\n",
    "        *[first(col_name).alias(col_name) for col_name in spark_df.columns if col_name != \"time\"]\n",
    "    )\n",
    "\n",
    "    # Bước 3: Thêm cột thời gian đại diện nhóm\n",
    "    spark_df = spark_df.withColumn(\"time_group\", col(\"window.start\")).drop(\"window\")\n",
    "\n",
    "    # Bước 4: Tạo đặc trưng lịch sử\n",
    "    window_spec = Window.orderBy(\"time_group\")\n",
    "    for i in range(1, lag_days + 1):    # ✅ Dùng lại biến mới tại đây\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "            spark_df = spark_df.withColumn(f\"{col_name}_b_{i}\", lag(col_name, i).over(window_spec))\n",
    "\n",
    "    # Bước 5: Loại bỏ các hàng thiếu dữ liệu lịch sử\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # Bước 6: Tạo nhãn NEXT_CLOSE\n",
    "    spark_df = spark_df.withColumn(\"NEXT_CLOSE\", lead(\"close\", 1).over(window_spec))\n",
    "\n",
    "    # Bước 7: Loại bỏ hàng cuối cùng không có nhãn\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # Bước 8: Loại bỏ các cột không cần thiết\n",
    "    spark_df = spark_df.drop(\"symbol\", \"time\", \"time_group\")\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "def split_data(spark_df, prediction_days=750):\n",
    "    total_rows = spark_df.count()\n",
    "    train_rows = total_rows - prediction_days\n",
    "    train_df = spark_df.limit(train_rows)\n",
    "    test_df = spark_df.subtract(train_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def transform_to_labeledpoint(df: DataFrame):\n",
    "    return df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "\n",
    "def train_random_forest_regressor(train_rdd, num_trees=1000, max_depth=7, impurity=\"variance\", max_bins=300, seed=13579):\n",
    "    model = RandomForest.trainRegressor(\n",
    "        data=train_rdd,\n",
    "        categoricalFeaturesInfo={},\n",
    "        numTrees=num_trees,\n",
    "        featureSubsetStrategy=\"auto\",\n",
    "        impurity=impurity,\n",
    "        maxDepth=max_depth,\n",
    "        maxBins=max_bins,\n",
    "        seed=seed\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_linear_regression(train_df, test_df):\n",
    "    feature_cols = train_df.columns[:-1]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_df).select(\"features\", \"NEXT_CLOSE\")\n",
    "    test_data = assembler.transform(test_df).select(\"features\", \"NEXT_CLOSE\")\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"NEXT_CLOSE\")\n",
    "    model = lr.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    evaluator = RegressionEvaluator(labelCol=\"NEXT_CLOSE\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    r2 = RegressionEvaluator(labelCol=\"NEXT_CLOSE\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(predictions)\n",
    "    return rmse, r2\n",
    "\n",
    "def train_xgboost(train_df, test_df):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"]\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"]\n",
    "    model = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, objective=\"reg:squarederror\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds) ** 0.5  # manually take square root\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    return rmse, r2\n",
    "\n",
    "def evaluate_random_forest_model(model, test_rdd):\n",
    "    preds = model.predict(test_rdd.map(lambda x: x.features))\n",
    "    pred_label_rdd = preds.zip(test_rdd.map(lambda x: x.label))\n",
    "    metrics = RegressionMetrics(pred_label_rdd)\n",
    "    return metrics.rootMeanSquaredError, metrics.r2\n",
    "\n",
    "# Proceed to execute the training for 10 crypto symbols\n",
    "spark = SparkSession.builder.appName(\"CryptoPredict\").getOrCreate()\n",
    "\n",
    "symbols = [\"BNBUSDT\", \"BTCUSDT\", \"ETHUSDT\", \"XRPUSDT\", \"SOLUSDT\",\n",
    "           \"LTCUSDT\", \"ETCUSDT\", \"PEPEUSDT\", \"DOGEUSDT\", \"ADAUSDT\"]\n",
    "\n",
    "def train_all_models_for_one_symbol(symbol: str):\n",
    "    print(f\"\\n🚀 Training models for: {symbol}\")\n",
    "    df = load_data_from_postgres(symbol)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"⚠️ Skipping {symbol} due to empty data.\")\n",
    "        return None\n",
    "\n",
    "    # 💡 Auto-scale small value coins\n",
    "    price_median = df[\"close\"].median()\n",
    "    scale_factor = 1.0\n",
    "    if price_median < 0.01:  # tuỳ chỉnh ngưỡng này\n",
    "        scale_factor = 1e6\n",
    "        print(f\"🔧 Scaling {symbol} values by {scale_factor} due to small price.\")\n",
    "\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "            df[col_name] = df[col_name] * scale_factor\n",
    "        df[\"volume\"] = df[\"volume\"]  # không scale volume\n",
    "    else:\n",
    "        print(f\"✅ No scaling needed for {symbol} (median close = {price_median})\")\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    processed_df = prepare_timeseries_data(spark_df)\n",
    "    train_df, test_df = split_data(processed_df)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        rmse, r2 = train_linear_regression(train_df, test_df)\n",
    "        results[\"linear_regression\"] = {\"rmse\": rmse, \"r2\": r2}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Linear Regression failed for {symbol}: {e}\")\n",
    "        results[\"linear_regression\"] = {\"rmse\": None, \"r2\": None}\n",
    "\n",
    "    try:\n",
    "        rmse, r2 = train_xgboost(train_df, test_df)\n",
    "        results[\"xgboost\"] = {\"rmse\": rmse, \"r2\": r2}\n",
    "        if r2 < 0:\n",
    "            print(f\"⚠️ XGBoost R² < 0 for {symbol} — model might be worse than baseline.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ XGBoost failed for {symbol}: {e}\")\n",
    "        results[\"xgboost\"] = {\"rmse\": None, \"r2\": None}\n",
    "\n",
    "    try:\n",
    "        train_rdd = transform_to_labeledpoint(train_df)\n",
    "        test_rdd = transform_to_labeledpoint(test_df)\n",
    "        model = train_random_forest_regressor(train_rdd)\n",
    "        rmse, r2 = evaluate_random_forest_model(model, test_rdd)\n",
    "        results[\"random_forest\"] = {\"rmse\": rmse, \"r2\": r2}\n",
    "        if r2 < 0:\n",
    "            print(f\"⚠️ Random Forest R² < 0 for {symbol}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Random Forest failed for {symbol}: {e}\")\n",
    "        results[\"random_forest\"] = {\"rmse\": None, \"r2\": None}\n",
    "\n",
    "    return {\"symbol\": symbol, \"results\": results}\n",
    "\n",
    "\n",
    "# Run for all symbols and collect results\n",
    "all_results = []\n",
    "for sym in symbols:\n",
    "    result = train_all_models_for_one_symbol(sym)\n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "# Format results into a DataFrame\n",
    "\n",
    "records = []\n",
    "for result in all_results:\n",
    "    symbol = result[\"symbol\"]\n",
    "    for model, metrics in result[\"results\"].items():\n",
    "        records.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"model\": model,\n",
    "            \"rmse\": metrics[\"rmse\"],\n",
    "            \"r2\": metrics[\"r2\"]\n",
    "        })\n",
    "        print(f\"Model: {model}, Symbol: {symbol}, RMSE: {metrics['rmse']}, R2: {metrics['r2']}\")\n",
    "\n",
    "pd.DataFrame(records)\n",
    "# Format results into a DataFrame\n",
    "records = []\n",
    "for result in all_results:\n",
    "    symbol = result[\"symbol\"]\n",
    "    for model, metrics in result[\"results\"].items():\n",
    "        records.append({\n",
    "            \"symbol\": symbol,\n",
    "            \"model\": model,\n",
    "            \"rmse\": metrics[\"rmse\"],\n",
    "            \"r2\": metrics[\"r2\"]\n",
    "        })\n",
    "\n",
    "pd.DataFrame(records)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
