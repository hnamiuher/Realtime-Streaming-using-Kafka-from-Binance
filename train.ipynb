{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e5478c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary modules after kernel reset\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, root_mean_squared_error\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, window, first, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ea832982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the core functions due to kernel reset\n",
    "def connect_to_postgres():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"airflow\",\n",
    "            user=\"airflow\",\n",
    "            password=\"airflow\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(f\"âŒ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "    \n",
    "def load_data_from_postgres(symbol):\n",
    "    conn = connect_to_postgres()\n",
    "    if conn:\n",
    "        query = f\"SELECT * FROM crypto_data WHERE symbol = '{symbol}';\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ecf0418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Remove outliers using IQR method\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    for col_name in columns:\n",
    "        Q1 = df[col_name].quantile(0.25)\n",
    "        Q3 = df[col_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        before = len(df)\n",
    "        df = df[(df[col_name] >= lower_bound) & (df[col_name] <= upper_bound)]\n",
    "        after = len(df)\n",
    "        print(f\"ðŸ“‰ Removed {before - after} outliers from '{col_name}'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "00419a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timeseries_data(spark_df: DataFrame,\n",
    "                             start: datetime.datetime = datetime.datetime(2023, 1, 1, 0, 0, 0),\n",
    "                             end: datetime.datetime = datetime.datetime(2025, 10, 1, 0, 0, 0),\n",
    "                             window_duration: str = \"12 hours\",\n",
    "                             lag_days: int = 7) -> DataFrame:   # âœ… Äá»•i tÃªn biáº¿n táº¡i Ä‘Ã¢y\n",
    "\n",
    "    # BÆ°á»›c 1: Lá»c theo khoáº£ng thá»i gian\n",
    "    spark_df = spark_df.filter((col(\"time\") >= start) & (col(\"time\") <= end))\n",
    "    \n",
    "    # BÆ°á»›c 2: Gom nhÃ³m theo cá»­a sá»• thá»i gian\n",
    "    spark_df = spark_df.groupBy(\n",
    "        window(col(\"time\"), window_duration)\n",
    "    ).agg(\n",
    "        *[first(col_name).alias(col_name) for col_name in spark_df.columns if col_name != \"time\"]\n",
    "    )\n",
    "\n",
    "    # BÆ°á»›c 3: ThÃªm cá»™t thá»i gian Ä‘áº¡i diá»‡n nhÃ³m\n",
    "    spark_df = spark_df.withColumn(\"time_group\", col(\"window.start\")).drop(\"window\")\n",
    "\n",
    "    # BÆ°á»›c 4: Táº¡o Ä‘áº·c trÆ°ng lá»‹ch sá»­\n",
    "    window_spec = Window.orderBy(\"time_group\")\n",
    "    for i in range(1, lag_days + 1):    # âœ… DÃ¹ng láº¡i biáº¿n má»›i táº¡i Ä‘Ã¢y\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "            spark_df = spark_df.withColumn(f\"{col_name}_b_{i}\", lag(col_name, i).over(window_spec))\n",
    "\n",
    "    # BÆ°á»›c 5: Loáº¡i bá» cÃ¡c hÃ ng thiáº¿u dá»¯ liá»‡u lá»‹ch sá»­\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # BÆ°á»›c 6: Táº¡o nhÃ£n NEXT_CLOSE\n",
    "    spark_df = spark_df.withColumn(\"NEXT_CLOSE\", lead(\"close\", 1).over(window_spec))\n",
    "\n",
    "    # BÆ°á»›c 7: Loáº¡i bá» hÃ ng cuá»‘i cÃ¹ng khÃ´ng cÃ³ nhÃ£n\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # BÆ°á»›c 8: Loáº¡i bá» cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t\n",
    "    spark_df = spark_df.drop(\"symbol\", \"time\", \"time_group\")\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0ce4a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(spark_df, prediction_days=75):\n",
    "    total_rows = spark_df.count()\n",
    "    train_rows = total_rows - prediction_days\n",
    "    train_df = spark_df.limit(train_rows)\n",
    "    test_df = spark_df.subtract(train_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def transform_to_labeledpoint(df: DataFrame):\n",
    "    return df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a447f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_metrics(metrics, model_name=\"Model\"):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    num_iterations = len(metrics[\"mse_train\"])  # Sá»‘ lÆ°á»£ng iterations\n",
    "    x_axis = range(0, num_iterations)\n",
    "\n",
    "    plt.plot(x_axis, metrics[\"mse_train\"], label=f\"Train MSE\")\n",
    "    plt.plot(x_axis, metrics[\"mse_test\"], label=f\"Test MSE\")\n",
    "    plt.plot(x_axis, metrics[\"rmse_train\"], label=f\"Train RMSE\")\n",
    "    plt.plot(x_axis, metrics[\"rmse_test\"], label=f\"Test RMSE\")\n",
    "    plt.plot(x_axis, metrics[\"mape_train\"], label=f\"Train MAPE\")\n",
    "    plt.plot(x_axis, metrics[\"mape_test\"], label=f\"Test MAPE\")\n",
    "    plt.plot(x_axis, metrics[\"r2_train\"], label=f\"Train R2\")\n",
    "    plt.plot(x_axis, metrics[\"r2_test\"], label=f\"Test R2\")\n",
    "\n",
    "    plt.title(f'{model_name} Metrics Over Iterations')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "aec0a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def train_random_forest(train_df, test_df, plot=True):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"]\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"]\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=6)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    mse_train = mean_squared_error(y_train, train_preds)\n",
    "    mse_test = mean_squared_error(y_test, test_preds)\n",
    "    rmse_train = mse_train ** 0.5\n",
    "    rmse_test = mse_test ** 0.5\n",
    "    mape_train = mean_absolute_percentage_error(y_train, train_preds)\n",
    "    mape_test = mean_absolute_percentage_error(y_test, test_preds)\n",
    "    r2_train = r2_score(y_train, train_preds)\n",
    "    r2_test = r2_score(y_test, test_preds)\n",
    "\n",
    "    metrics = {\n",
    "        \"mse\": (mse_train, mse_test),\n",
    "        \"rmse\": (rmse_train, rmse_test),\n",
    "        \"mape\": (mape_train, mape_test),\n",
    "        \"r2\": (r2_train, r2_test)\n",
    "    }\n",
    "\n",
    "    # if plot:\n",
    "    #     plot_train_test_metrics(metrics, model_name=\"Random Forest\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cd79ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_forest_model_train_test(model, train_rdd, test_rdd, plot=True):\n",
    "    train_preds = list(model.predict(train_rdd.map(lambda x: x.features)).collect())\n",
    "    train_labels = train_rdd.map(lambda x: x.label).collect()\n",
    "\n",
    "    test_preds = list(model.predict(test_rdd.map(lambda x: x.features)).collect())\n",
    "    test_labels = test_rdd.map(lambda x: x.label).collect()\n",
    "\n",
    "    metrics = {\n",
    "        \"mse\": (\n",
    "            mean_squared_error(train_labels, train_preds),\n",
    "            mean_squared_error(test_labels, test_preds)\n",
    "        ),\n",
    "        \"rmse\": (\n",
    "            root_mean_squared_error(train_labels, train_preds, squared=False),\n",
    "            root_mean_squared_error(test_labels, test_preds, squared=False)\n",
    "        ),\n",
    "        \"mape\": (\n",
    "            mean_absolute_percentage_error(train_labels, train_preds),\n",
    "            mean_absolute_percentage_error(test_labels, test_preds)\n",
    "        ),\n",
    "        \"r2\": (\n",
    "            r2_score(train_labels, train_preds),\n",
    "            r2_score(test_labels, test_preds)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # if plot:\n",
    "    #     plot_train_test_metrics(metrics, model_name=\"Random Forest\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "734130dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train_linear_regression(train_df, test_df):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"]).values  # Chuyá»ƒn sang numpy array\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"].values  # Chuyá»ƒn sang numpy array\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"]).values  # Chuyá»ƒn sang numpy array\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"].values  # Chuyá»ƒn sang numpy array\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Dá»± Ä‘oÃ¡n trÃªn train vÃ  test set\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"mse\": (\n",
    "            mean_squared_error(y_train, train_preds),\n",
    "            mean_squared_error(y_test, test_preds)\n",
    "        ),\n",
    "        \"rmse\": (\n",
    "            root_mean_squared_error(y_train, train_preds),\n",
    "            root_mean_squared_error(y_test, test_preds)\n",
    "        ),\n",
    "        \"mape\": (\n",
    "            mean_absolute_percentage_error(y_train, train_preds),\n",
    "            mean_absolute_percentage_error(y_test, test_preds)\n",
    "        ),\n",
    "        \"r2\": (\n",
    "            r2_score(y_train, train_preds),\n",
    "            r2_score(y_test, test_preds)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "49a14490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgboost(train_df, test_df, plot=True):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"]).values\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"].values\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"]).values\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"].values\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective=\"reg:squarederror\"\n",
    "    )\n",
    "\n",
    "    evals_result = {}  # Dictionary Ä‘á»ƒ lÆ°u trá»¯ káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        eval_metric=[\"rmse\", \"mse\", \"mape\", \"r2\"],  # YÃªu cáº§u tÃ­nh toÃ¡n cÃ¡c metrics\n",
    "        callbacks=[xgb.callback.record_evaluation(evals_result)], # Ghi láº¡i káº¿t quáº£ vÃ o biáº¿n evals_result\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mse_train\": evals_result[\"validation_0\"][\"mse\"],\n",
    "        \"mse_test\": evals_result[\"validation_1\"][\"mse\"],\n",
    "        \"rmse_train\": evals_result[\"validation_0\"][\"rmse\"],\n",
    "        \"rmse_test\": evals_result[\"validation_1\"][\"rmse\"],\n",
    "        \"mape_train\": evals_result[\"validation_0\"][\"mape\"],\n",
    "        \"mape_test\": evals_result[\"validation_1\"][\"mape\"],\n",
    "        \"r2_train\": evals_result[\"validation_0\"][\"r2\"],\n",
    "        \"r2_test\": evals_result[\"validation_1\"][\"r2\"],\n",
    "    }\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed to execute the training for 10 crypto symbols\n",
    "spark = SparkSession.builder.appName(\"CryptoPredict\").getOrCreate()\n",
    "\n",
    "symbols = [\"BNBUSDT\", \"BTCUSDT\", \"ETHUSDT\", \"XRPUSDT\", \"SOLUSDT\",\n",
    "           \"LTCUSDT\", \"ETCUSDT\", \"PEPEUSDT\", \"DOGEUSDT\", \"ADAUSDT\"]\n",
    "\n",
    "# Äoáº¡n mÃ£ train_all_models_for_one_symbol\n",
    "def train_all_models_for_one_symbol(symbol: str):\n",
    "    print(f\"\\nðŸš€ Training models for: {symbol}\")\n",
    "    df = load_data_from_postgres(symbol)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"âš ï¸ Skipping {symbol} due to empty data.\")\n",
    "        return None\n",
    "    df = remove_outliers_iqr(df, [\"close\"])\n",
    "\n",
    "    # ðŸ’¡ Auto-scale small value coins\n",
    "    price_median = df[\"close\"].median()\n",
    "    scale_factor = 1.0\n",
    "    if price_median < 0.01:\n",
    "        scale_factor = 1e6\n",
    "        print(f\"ðŸ”§ Scaling {symbol} values by {scale_factor} due to small price.\")\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "            df[col_name] = df[col_name] * scale_factor\n",
    "        df[\"volume\"] = df[\"volume\"]  # khÃ´ng scale volume\n",
    "    else:\n",
    "        print(f\"âœ… No scaling needed for {symbol} (median close = {price_median})\")\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    processed_df = prepare_timeseries_data(spark_df)\n",
    "    train_df, test_df = split_data(processed_df)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        metrics = train_linear_regression(train_df, test_df)\n",
    "        results[\"linear_regression\"] = metrics\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Linear Regression failed for {symbol}: {e}\")\n",
    "        results[\"linear_regression\"] = {\"rmse\": None, \"r2\": None, \"mse\": None, \"mape\": None}\n",
    "\n",
    "    try:\n",
    "        metrics = train_xgboost(train_df, test_df)\n",
    "        results[\"xgboost\"] = metrics\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ XGBoost failed for {symbol}: {e}\")\n",
    "        results[\"xgboost\"] = {\"rmse\": None, \"r2\": None, \"mse\": None, \"mape\": None}\n",
    "\n",
    "    try:\n",
    "        metrics = train_random_forest(train_df, test_df)\n",
    "        results[\"random_forest\"] = metrics\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Random Forest failed for {symbol}: {e}\")\n",
    "        results[\"random_forest\"] = {\"rmse\": None, \"r2\": None, \"mse\": None, \"mape\": None}\n",
    "\n",
    "    return {\"symbol\": symbol, \"results\": results}\n",
    "\n",
    "# In káº¿t quáº£\n",
    "for sym in symbols:\n",
    "    result = train_all_models_for_one_symbol(sym)\n",
    "    if result:\n",
    "        for model, metrics in result[\"results\"].items():\n",
    "            print(f\"Model: {model}, Symbol: {result['symbol']}, \"\n",
    "                  f\"RMSE: {metrics['rmse']}, MSE: {metrics['mse']}, \"\n",
    "                  f\"MAPE: {metrics['mape']}, R2: {metrics['r2']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_close(symbol: str):\n",
    "    print(f\"\\nðŸ”® Predicting next CLOSE for: {symbol}\")\n",
    "    df = load_data_from_postgres(symbol)\n",
    "    print(df.head())\n",
    "    print(f\"ðŸ“Š Loaded data for {symbol}: {len(df)} rows\")\n",
    "    if df is None or len(df) < 8:\n",
    "        print(f\"âš ï¸ Not enough data for prediction: {symbol}\")\n",
    "        return None\n",
    "\n",
    "    df = df.sort_values(\"time\", ascending=False).head(10000).sort_values(\"time\")\n",
    "    df = remove_outliers_iqr(df, [\"close\"])\n",
    "\n",
    "    # Dá»±a vÃ o cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ train trÆ°á»›c Ä‘Ã³, Ä‘Æ°a ra dá»± Ä‘oÃ¡n cho giÃ¡ CLOSE tiáº¿p theo báº±ng cáº£ 3 mÃ´ hÃ¬nh\n",
    "    # 1. Linear Regression khÃ´ng cáº§n fit láº¡i\n",
    "    linear = train_linear_regression(df, df)\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(df.drop(columns=[\"NEXT_CLOSE\"]).values, df[\"NEXT_CLOSE\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_next_close(\"BTCUSDT\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training models for: BTCUSDT with anti-overfitting techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoain\\AppData\\Local\\Temp\\ipykernel_6588\\59782643.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‰ Removed 0 outliers from 'close'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\hoain\\\\AppData\\\\Local\\\\Temp\\\\spark-51e85a69-8968-4163-a5fc-a687b3b91632\\\\pyspark-4cf919fa-428c-4088-9e01-3cc48d0bfd87\\\\tmpk30vednf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 372\u001b[39m\n\u001b[32m    370\u001b[39m symbols = [\u001b[33m\"\u001b[39m\u001b[33mBTCUSDT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mETHUSDT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBNBUSDT\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m symbols:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     results = \u001b[43mtrain_all_models_with_anti_overfitting\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# Save models for later use\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 332\u001b[39m, in \u001b[36mtrain_all_models_with_anti_overfitting\u001b[39m\u001b[34m(symbol)\u001b[39m\n\u001b[32m    329\u001b[39m         df[col_name] = df[col_name] * scale_factor\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Create Spark DataFrame\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m spark_df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Use time series cross-validation\u001b[39;00m\n\u001b[32m    335\u001b[39m cv_results = time_series_cross_validation(\n\u001b[32m    336\u001b[39m     spark_df, \n\u001b[32m    337\u001b[39m     n_splits=\u001b[32m5\u001b[39m,\n\u001b[32m    338\u001b[39m     prediction_window=\u001b[33m\"\u001b[39m\u001b[33m1 hour\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Use daily predictions instead of hourly\u001b[39;00m\n\u001b[32m    339\u001b[39m     lag_periods=\u001b[32m7\u001b[39m               \u001b[38;5;66;03m# Use fewer lag periods\u001b[39;00m\n\u001b[32m    340\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1440\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1436\u001b[39m     data = pd.DataFrame(data, columns=column_names)\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[32m   1441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_dataframe(\n\u001b[32m   1444\u001b[39m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1445\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:363\u001b[39m, in \u001b[36mSparkConversionMixin.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    361\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    362\u001b[39m converted_data = \u001b[38;5;28mself\u001b[39m._convert_from_pandas(data, schema, timezone)\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1483\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     rdd, struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1116\u001b[39m, in \u001b[36mSparkSession._createFromLocal\u001b[39m\u001b[34m(self, data, schema)\u001b[39m\n\u001b[32m   1114\u001b[39m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[32m   1115\u001b[39m internal_data = [struct.toInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:824\u001b[39m, in \u001b[36mSparkContext.parallelize\u001b[39m\u001b[34m(self, c, numSlices)\u001b[39m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    822\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm.PythonParallelizeServer(\u001b[38;5;28mself\u001b[39m._jsc.sc(), numSlices)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m jrdd = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:864\u001b[39m, in \u001b[36mSparkContext._serialize_to_jvm\u001b[39m\u001b[34m(self, data, serializer, reader_func, server_func)\u001b[39m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# without encryption, we serialize to a file, and we read the file in java and\u001b[39;00m\n\u001b[32m    863\u001b[39m     \u001b[38;5;66;03m# parallelize from there.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m     tempFile = \u001b[43mNamedTemporaryFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelete\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_temp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    866\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py:563\u001b[39m, in \u001b[36mNamedTemporaryFile\u001b[39m\u001b[34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     file = _io.open(\u001b[38;5;28mdir\u001b[39m, mode, buffering=buffering,\n\u001b[32m    564\u001b[39m                     newline=newline, encoding=encoding, errors=errors,\n\u001b[32m    565\u001b[39m                     opener=opener)\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    567\u001b[39m         raw = \u001b[38;5;28mgetattr\u001b[39m(file, \u001b[33m'\u001b[39m\u001b[33mbuffer\u001b[39m\u001b[33m'\u001b[39m, file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py:560\u001b[39m, in \u001b[36mNamedTemporaryFile.<locals>.opener\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mopener\u001b[39m(*args):\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mnonlocal\u001b[39;00m name\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     fd, name = \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py:256\u001b[39m, in \u001b[36m_mkstemp_inner\u001b[39m\u001b[34m(dir, pre, suf, flags, output_type)\u001b[39m\n\u001b[32m    254\u001b[39m _sys.audit(\u001b[33m\"\u001b[39m\u001b[33mtempfile.mkstemp\u001b[39m\u001b[33m\"\u001b[39m, file)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     fd = _os.open(file, flags, \u001b[32m0o600\u001b[39m)\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\hoain\\\\AppData\\\\Local\\\\Temp\\\\spark-51e85a69-8968-4163-a5fc-a687b3b91632\\\\pyspark-4cf919fa-428c-4088-9e01-3cc48d0bfd87\\\\tmpk30vednf'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Libraries\n",
    "# import numpy as np  # to handle matrix\n",
    "# import pandas as pd # to handle data\n",
    "# from matplotlib import pyplot as plt # to visualize\n",
    "# import datetime, pytz # to handle time\n",
    "# from sklearn.model_selection import train_test_split # Split data\n",
    "# from sklearn.ensemble import RandomForestRegressor # Random Forest Classifier\n",
    "# import logging\n",
    "# import psycopg2\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# import psycopg2\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, lag, avg\n",
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "# from pyspark.ml.regression import GBTRegressor\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.sql.functions import abs as sql_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def connect_to_postgres():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=\"airflow\",\n",
    "#             user=\"airflow\",\n",
    "#             password=\"airflow\",\n",
    "#             host=\"localhost\",\n",
    "#             port=\"5432\"\n",
    "#         )\n",
    "#         logging.info(\"âœ… Connected to PostgreSQL.\")\n",
    "#         return conn\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"âŒ Failed to connect to PostgreSQL: {e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77133202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Táº¡o Spark session vÃ  connect to PostgreSQL báº±ng cÃ¡ch cháº¡y hÃ m connect_to_postgres\n",
    "# spark = SparkSession.builder.master(\"local\").appName(\"Crypto\").getOrCreate()\n",
    "# conn = connect_to_postgres()\n",
    "# # Náº¿u káº¿t ná»‘i thÃ nh cÃ´ng, in ra dataframe tá»« báº£ng crypto_prices\n",
    "# if conn:\n",
    "#     # Read data from PostgreSQL\n",
    "#     query = \"SELECT * FROM crypto_data WHERE symbol = 'BNBUSDT';\"\n",
    "#     df = pd.read_sql(query, conn)\n",
    "#     # Convert the DataFrame to a Spark DataFrame\n",
    "#     spark_df = spark.createDataFrame(df)\n",
    "#     # Close the connection\n",
    "#     conn.close()\n",
    "#     # In ra 5 dÃ²ng Ä‘áº§u tiÃªn cá»§a dataframe\n",
    "#     spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In ra shape cá»§a dataframe\n",
    "# print(f\"Shape of DataFrame: {spark_df.count()} rows, {len(spark_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, window, first\n",
    "# import datetime\n",
    "\n",
    "# start = datetime.datetime(2023, 1, 1, 0, 0, 0)\n",
    "# end = datetime.datetime(2025, 10, 1, 0, 0, 0)\n",
    "\n",
    "# # Filter theo khoáº£ng thá»i gian\n",
    "# spark_df = spark_df.filter((col(\"time\") >= start) & (col(\"time\") <= end))\n",
    "\n",
    "# # Group by theo cá»­a sá»• thá»i gian 1 giá»\n",
    "# spark_df = spark_df.groupBy(\n",
    "#     window(col(\"time\"), \"3 hours\")\n",
    "# ).agg(\n",
    "#     *[first(col_name).alias(col_name) for col_name in spark_df.columns if col_name != \"time\"]\n",
    "# )\n",
    "\n",
    "# # Náº¿u muá»‘n hiá»ƒn thá»‹ thá»i gian báº¯t Ä‘áº§u má»‘c group\n",
    "# spark_df = spark_df.withColumn(\"time_group\", col(\"window.start\")).drop(\"window\")\n",
    "\n",
    "# # Hiá»ƒn thá»‹ 3 dÃ²ng Ä‘áº§u\n",
    "# spark_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa241202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lag\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # XÃ¡c Ä‘á»‹nh cá»­a sá»• Ä‘á»ƒ tÃ­nh toÃ¡n lag\n",
    "# window_spec = Window.orderBy(\"time_group\")\n",
    "\n",
    "# # ThÃªm cÃ¡c cá»™t lá»‹ch sá»­ (lag features) cho 7 ngÃ y\n",
    "# for i in range(1, 8):  # for 7 days\n",
    "#     spark_df = spark_df.withColumn(f\"open_b_{i}\", lag(\"open\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"high_b_{i}\", lag(\"high\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"low_b_{i}\", lag(\"low\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"close_b_{i}\", lag(\"close\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"volume_b_{i}\", lag(\"volume\", i).over(window_spec))\n",
    "\n",
    "# # Loáº¡i bá» cÃ¡c hÃ ng khÃ´ng Ä‘á»§ dá»¯ liá»‡u lá»‹ch sá»­\n",
    "# spark_df = spark_df.dropna()\n",
    "\n",
    "# # Hiá»ƒn thá»‹ thÃ´ng tin vá» dá»¯ liá»‡u sau khi thÃªm cá»™t lá»‹ch sá»­\n",
    "# print(f\"Historical Data Shape: {spark_df.count()} rows, {len(spark_df.columns)} columns\")\n",
    "# spark_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f935b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lead\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # XÃ¡c Ä‘á»‹nh cá»­a sá»• Ä‘á»ƒ tÃ­nh toÃ¡n giÃ¡ trá»‹ tiáº¿p theo\n",
    "# window_spec = Window.orderBy(\"time_group\")\n",
    "\n",
    "# # ThÃªm cá»™t \"NEXT_CLOSE\" lÃ m nhÃ£n (label) cho giÃ¡ trá»‹ Ä‘Ã³ng cá»­a tiáº¿p theo\n",
    "# spark_df = spark_df.withColumn(\"NEXT_CLOSE\", lead(\"close\", 1).over(window_spec))\n",
    "\n",
    "# # Loáº¡i bá» cÃ¡c hÃ ng khÃ´ng Ä‘á»§ dá»¯ liá»‡u (hÃ ng cuá»‘i cÃ¹ng khÃ´ng cÃ³ giÃ¡ trá»‹ tiáº¿p theo)\n",
    "# spark_df = spark_df.dropna()\n",
    "\n",
    "# # Loáº¡i bá» cá»™t symbol vÃ  time vÃ¬ khÃ´ng cáº§n thiáº¿t cho mÃ´ hÃ¬nh\n",
    "# spark_df = spark_df.drop(\"symbol\", \"time\", \"time_group\")\n",
    "\n",
    "\n",
    "# # Hiá»ƒn thá»‹ thÃ´ng tin vá» dá»¯ liá»‡u sau khi thÃªm cá»™t nhÃ£n\n",
    "# print(f\"After adding NEXT_CLOSE Label, new shape: {spark_df.count()} rows, {len(spark_df.columns)} columns\")\n",
    "# spark_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sá»‘ ngÃ y dá»± Ä‘oÃ¡n\n",
    "# prediction_days = 250\n",
    "# # Tá»•ng sá»‘ hÃ ng trong DataFrame\n",
    "# total_rows = spark_df.count()\n",
    "\n",
    "# # Sá»‘ hÃ ng cho táº­p huáº¥n luyá»‡n\n",
    "# train_rows = total_rows - prediction_days\n",
    "\n",
    "# # Chia dá»¯ liá»‡u thÃ nh táº­p train vÃ  test\n",
    "# train_df = spark_df.limit(train_rows)  # Láº¥y `train_rows` Ä‘áº§u tiÃªn lÃ m táº­p huáº¥n luyá»‡n\n",
    "# test_df = spark_df.subtract(train_df)  # Pháº§n cÃ²n láº¡i lÃ  táº­p kiá»ƒm tra\n",
    "\n",
    "# # Hiá»ƒn thá»‹ thÃ´ng tin vá» táº­p train/test\n",
    "# print(f\"PERCENT test/total data = {(prediction_days / total_rows) * 100:.2f}%\")\n",
    "# print(f\"Train data shape: {train_df.count()} rows, {len(train_df.columns)} columns\")\n",
    "# print(f\"Test data shape: {test_df.count()} rows, {len(test_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_TrainingData = train_df\n",
    "# spark_TestData = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.regression import LabeledPoint\n",
    "# from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# transformed_TrainingData = spark_TrainingData.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "# transformed_TestData = spark_TestData.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of training set rows: %d\" % transformed_TrainingData.count())\n",
    "# print(\"Number of test set rows: %d\" % transformed_TestData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad97ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's make sure we have the correct types.\n",
    "# print(\"%s should be an RDD\" % type(transformed_TrainingData))\n",
    "# print(\"%s should be a LabeledPoint\" % type(transformed_TestData.first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.tree import RandomForest\n",
    "# from time import *\n",
    "# RF_NUM_TREES = 1000 # 5\n",
    "# RF_MAX_DEPTH = 7 # 5\n",
    "# IMPURITY =\"variance\"\n",
    "# RF_MAX_BINS = 300\n",
    "# RANDOM_SEED = 13579 #None\n",
    "\n",
    "# start_time = time()\n",
    "\n",
    "# model = RandomForest.trainRegressor(transformed_TrainingData, categoricalFeaturesInfo={}, \\\n",
    "#     numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity= IMPURITY, \\\n",
    "#     maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=RANDOM_SEED)\n",
    "\n",
    "# end_time = time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions\n",
    "# predictions = model.predict(transformed_TestData.map(lambda x: x.features))\n",
    "\n",
    "# # Repartition both RDDs to ensure they have the same number of partitions\n",
    "# labels = transformed_TestData.map(lambda x: x.label).repartition(1)\n",
    "# predictions = predictions.repartition(1)\n",
    "\n",
    "# # Zip the RDDs after ensuring they are aligned\n",
    "# labels_and_predictions = labels.zip(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute accuracy\n",
    "# from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# start_time = time()\n",
    "\n",
    "# metrics = RegressionMetrics(labels_and_predictions)\n",
    "# print(\"Root Mean Squared Error: %.f\" % (metrics.rootMeanSquaredError))\n",
    "# print(\"R^2 Error: %.3f\" % (metrics.r2))\n",
    "\n",
    "# end_time = time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(\"Time to evaluate model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_Real = transformed_TestData.map(lambda x: x.label).collect()\n",
    "# list_Predicted = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb94a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualising the results\n",
    "# plt.figure(figsize=(25,15), dpi=80, facecolor='w', edgecolor='k')\n",
    "# ax = plt.gca()  \n",
    "# # Convert test_df to Pandas DataFrame to access the index\n",
    "# test_df_pd = test_df.toPandas()\n",
    "# plt.plot(test_df_pd.index, list_Real, color='red', label='Real BTC Price')\n",
    "# plt.plot(test_df_pd.index, list_Predicted, color='blue', label='Predicted BTC Price')\n",
    "# plt.title('BTC Price Prediction', fontsize=40)\n",
    "# x = test_df_pd.reset_index().index\n",
    "# for tick in ax.xaxis.get_major_ticks():\n",
    "#     tick.label1.set_fontsize(18)\n",
    "# for tick in ax.yaxis.get_major_ticks():\n",
    "#     tick.label1.set_fontsize(18)\n",
    "# plt.xlabel('Time', fontsize=40)\n",
    "# plt.ylabel('BTC Price(USD) [Closed]', fontsize=40)\n",
    "# plt.legend(loc=2, prop={'size': 25})\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
