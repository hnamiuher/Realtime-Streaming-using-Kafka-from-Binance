{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e5478c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary modules after kernel reset\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import psycopg2\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, root_mean_squared_error\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, window, first, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.evaluation import RegressionMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ea832982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-define the core functions due to kernel reset\n",
    "def connect_to_postgres():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"airflow\",\n",
    "            user=\"airflow\",\n",
    "            password=\"airflow\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "        return None\n",
    "    \n",
    "def load_data_from_postgres(symbol):\n",
    "    conn = connect_to_postgres()\n",
    "    if conn:\n",
    "        query = f\"SELECT * FROM crypto_data WHERE symbol = '{symbol}';\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        conn.close()\n",
    "        return df\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ecf0418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Remove outliers using IQR method\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    for col_name in columns:\n",
    "        Q1 = df[col_name].quantile(0.25)\n",
    "        Q3 = df[col_name].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        before = len(df)\n",
    "        df = df[(df[col_name] >= lower_bound) & (df[col_name] <= upper_bound)]\n",
    "        after = len(df)\n",
    "        print(f\"📉 Removed {before - after} outliers from '{col_name}'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "00419a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timeseries_data(spark_df: DataFrame,\n",
    "                             start: datetime.datetime = datetime.datetime(2023, 1, 1, 0, 0, 0),\n",
    "                             end: datetime.datetime = datetime.datetime(2025, 10, 1, 0, 0, 0),\n",
    "                             window_duration: str = \"12 hours\",\n",
    "                             lag_days: int = 7) -> DataFrame:   # ✅ Đổi tên biến tại đây\n",
    "\n",
    "    # Bước 1: Lọc theo khoảng thời gian\n",
    "    spark_df = spark_df.filter((col(\"time\") >= start) & (col(\"time\") <= end))\n",
    "    \n",
    "    # Bước 2: Gom nhóm theo cửa sổ thời gian\n",
    "    spark_df = spark_df.groupBy(\n",
    "        window(col(\"time\"), window_duration)\n",
    "    ).agg(\n",
    "        *[first(col_name).alias(col_name) for col_name in spark_df.columns if col_name != \"time\"]\n",
    "    )\n",
    "\n",
    "    # Bước 3: Thêm cột thời gian đại diện nhóm\n",
    "    spark_df = spark_df.withColumn(\"time_group\", col(\"window.start\")).drop(\"window\")\n",
    "\n",
    "    # Bước 4: Tạo đặc trưng lịch sử\n",
    "    window_spec = Window.orderBy(\"time_group\")\n",
    "    for i in range(1, lag_days + 1):    # ✅ Dùng lại biến mới tại đây\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "            spark_df = spark_df.withColumn(f\"{col_name}_b_{i}\", lag(col_name, i).over(window_spec))\n",
    "\n",
    "    # Bước 5: Loại bỏ các hàng thiếu dữ liệu lịch sử\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # Bước 6: Tạo nhãn NEXT_CLOSE\n",
    "    spark_df = spark_df.withColumn(\"NEXT_CLOSE\", lead(\"close\", 1).over(window_spec))\n",
    "\n",
    "    # Bước 7: Loại bỏ hàng cuối cùng không có nhãn\n",
    "    spark_df = spark_df.dropna()\n",
    "\n",
    "    # Bước 8: Loại bỏ các cột không cần thiết\n",
    "    spark_df = spark_df.drop(\"symbol\", \"time\", \"time_group\")\n",
    "\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0ce4a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(spark_df, prediction_days=75):\n",
    "    total_rows = spark_df.count()\n",
    "    train_rows = total_rows - prediction_days\n",
    "    train_df = spark_df.limit(train_rows)\n",
    "    test_df = spark_df.subtract(train_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "def transform_to_labeledpoint(df: DataFrame):\n",
    "    return df.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a447f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_metrics(metrics, model_name=\"Model\"):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    num_iterations = len(metrics[\"mse_train\"])  # Số lượng iterations\n",
    "    x_axis = range(0, num_iterations)\n",
    "\n",
    "    plt.plot(x_axis, metrics[\"mse_train\"], label=f\"Train MSE\")\n",
    "    plt.plot(x_axis, metrics[\"mse_test\"], label=f\"Test MSE\")\n",
    "    plt.plot(x_axis, metrics[\"rmse_train\"], label=f\"Train RMSE\")\n",
    "    plt.plot(x_axis, metrics[\"rmse_test\"], label=f\"Test RMSE\")\n",
    "    plt.plot(x_axis, metrics[\"mape_train\"], label=f\"Train MAPE\")\n",
    "    plt.plot(x_axis, metrics[\"mape_test\"], label=f\"Test MAPE\")\n",
    "    plt.plot(x_axis, metrics[\"r2_train\"], label=f\"Train R2\")\n",
    "    plt.plot(x_axis, metrics[\"r2_test\"], label=f\"Test R2\")\n",
    "\n",
    "    plt.title(f'{model_name} Metrics Over Iterations')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "aec0a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def train_random_forest(train_df, test_df, plot=True):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"]\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"])\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"]\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=6)\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    mse_train = mean_squared_error(y_train, train_preds)\n",
    "    mse_test = mean_squared_error(y_test, test_preds)\n",
    "    rmse_train = mse_train ** 0.5\n",
    "    rmse_test = mse_test ** 0.5\n",
    "    mape_train = mean_absolute_percentage_error(y_train, train_preds)\n",
    "    mape_test = mean_absolute_percentage_error(y_test, test_preds)\n",
    "    r2_train = r2_score(y_train, train_preds)\n",
    "    r2_test = r2_score(y_test, test_preds)\n",
    "\n",
    "    metrics = {\n",
    "        \"mse\": (mse_train, mse_test),\n",
    "        \"rmse\": (rmse_train, rmse_test),\n",
    "        \"mape\": (mape_train, mape_test),\n",
    "        \"r2\": (r2_train, r2_test)\n",
    "    }\n",
    "\n",
    "    # if plot:\n",
    "    #     plot_train_test_metrics(metrics, model_name=\"Random Forest\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cd79ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_forest_model_train_test(model, train_rdd, test_rdd, plot=True):\n",
    "    train_preds = list(model.predict(train_rdd.map(lambda x: x.features)).collect())\n",
    "    train_labels = train_rdd.map(lambda x: x.label).collect()\n",
    "\n",
    "    test_preds = list(model.predict(test_rdd.map(lambda x: x.features)).collect())\n",
    "    test_labels = test_rdd.map(lambda x: x.label).collect()\n",
    "\n",
    "    metrics = {\n",
    "        \"mse\": (\n",
    "            mean_squared_error(train_labels, train_preds),\n",
    "            mean_squared_error(test_labels, test_preds)\n",
    "        ),\n",
    "        \"rmse\": (\n",
    "            root_mean_squared_error(train_labels, train_preds, squared=False),\n",
    "            root_mean_squared_error(test_labels, test_preds, squared=False)\n",
    "        ),\n",
    "        \"mape\": (\n",
    "            mean_absolute_percentage_error(train_labels, train_preds),\n",
    "            mean_absolute_percentage_error(test_labels, test_preds)\n",
    "        ),\n",
    "        \"r2\": (\n",
    "            r2_score(train_labels, train_preds),\n",
    "            r2_score(test_labels, test_preds)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # if plot:\n",
    "    #     plot_train_test_metrics(metrics, model_name=\"Random Forest\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "734130dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def train_linear_regression(train_df, test_df):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"]).values  # Chuyển sang numpy array\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"].values  # Chuyển sang numpy array\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"]).values  # Chuyển sang numpy array\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"].values  # Chuyển sang numpy array\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Huấn luyện mô hình\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Dự đoán trên train và test set\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"mse\": (\n",
    "            mean_squared_error(y_train, train_preds),\n",
    "            mean_squared_error(y_test, test_preds)\n",
    "        ),\n",
    "        \"rmse\": (\n",
    "            root_mean_squared_error(y_train, train_preds),\n",
    "            root_mean_squared_error(y_test, test_preds)\n",
    "        ),\n",
    "        \"mape\": (\n",
    "            mean_absolute_percentage_error(y_train, train_preds),\n",
    "            mean_absolute_percentage_error(y_test, test_preds)\n",
    "        ),\n",
    "        \"r2\": (\n",
    "            r2_score(y_train, train_preds),\n",
    "            r2_score(y_test, test_preds)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "49a14490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def train_xgboost(train_df, test_df, plot=True):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "    X_train = train_pd.drop(columns=[\"NEXT_CLOSE\"]).values\n",
    "    y_train = train_pd[\"NEXT_CLOSE\"].values\n",
    "    X_test = test_pd.drop(columns=[\"NEXT_CLOSE\"]).values\n",
    "    y_test = test_pd[\"NEXT_CLOSE\"].values\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        objective=\"reg:squarederror\"\n",
    "    )\n",
    "\n",
    "    evals_result = {}  # Dictionary để lưu trữ kết quả đánh giá\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        eval_metric=[\"rmse\", \"mse\", \"mape\", \"r2\"],  # Yêu cầu tính toán các metrics\n",
    "        callbacks=[xgb.callback.record_evaluation(evals_result)], # Ghi lại kết quả vào biến evals_result\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"mse_train\": evals_result[\"validation_0\"][\"mse\"],\n",
    "        \"mse_test\": evals_result[\"validation_1\"][\"mse\"],\n",
    "        \"rmse_train\": evals_result[\"validation_0\"][\"rmse\"],\n",
    "        \"rmse_test\": evals_result[\"validation_1\"][\"rmse\"],\n",
    "        \"mape_train\": evals_result[\"validation_0\"][\"mape\"],\n",
    "        \"mape_test\": evals_result[\"validation_1\"][\"mape\"],\n",
    "        \"r2_train\": evals_result[\"validation_0\"][\"r2\"],\n",
    "        \"r2_test\": evals_result[\"validation_1\"][\"r2\"],\n",
    "    }\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceed to execute the training for 10 crypto symbols\n",
    "spark = SparkSession.builder.appName(\"CryptoPredict\").getOrCreate()\n",
    "\n",
    "symbols = [\"BNBUSDT\", \"BTCUSDT\", \"ETHUSDT\", \"XRPUSDT\", \"SOLUSDT\",\n",
    "           \"LTCUSDT\", \"ETCUSDT\", \"PEPEUSDT\", \"DOGEUSDT\", \"ADAUSDT\"]\n",
    "\n",
    "# Đoạn mã train_all_models_for_one_symbol\n",
    "def train_all_models_for_one_symbol(symbol: str):\n",
    "    print(f\"\\n🚀 Training models for: {symbol}\")\n",
    "    df = load_data_from_postgres(symbol)\n",
    "    if df is None or df.empty:\n",
    "        print(f\"⚠️ Skipping {symbol} due to empty data.\")\n",
    "        return None\n",
    "    df = remove_outliers_iqr(df, [\"close\"])\n",
    "\n",
    "    # 💡 Auto-scale small value coins\n",
    "    price_median = df[\"close\"].median()\n",
    "    scale_factor = 1.0\n",
    "    if price_median < 0.01:\n",
    "        scale_factor = 1e6\n",
    "        print(f\"🔧 Scaling {symbol} values by {scale_factor} due to small price.\")\n",
    "        for col_name in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "            df[col_name] = df[col_name] * scale_factor\n",
    "        df[\"volume\"] = df[\"volume\"]  # không scale volume\n",
    "    else:\n",
    "        print(f\"✅ No scaling needed for {symbol} (median close = {price_median})\")\n",
    "\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    processed_df = prepare_timeseries_data(spark_df)\n",
    "    train_df, test_df = split_data(processed_df)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        metrics = train_linear_regression(train_df, test_df)\n",
    "        results[\"linear_regression\"] = metrics\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Linear Regression failed for {symbol}: {e}\")\n",
    "        results[\"linear_regression\"] = {\"rmse\": None, \"r2\": None, \"mse\": None, \"mape\": None}\n",
    "\n",
    "    try:\n",
    "        metrics = train_xgboost(train_df, test_df)\n",
    "        results[\"xgboost\"] = metrics\n",
    "    except Exception as e:\n",
    "        print(f\"❌ XGBoost failed for {symbol}: {e}\")\n",
    "        results[\"xgboost\"] = {\"rmse\": None, \"r2\": None, \"mse\": None, \"mape\": None}\n",
    "\n",
    "    try:\n",
    "        metrics = train_random_forest(train_df, test_df)\n",
    "        results[\"random_forest\"] = metrics\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Random Forest failed for {symbol}: {e}\")\n",
    "        results[\"random_forest\"] = {\"rmse\": None, \"r2\": None, \"mse\": None, \"mape\": None}\n",
    "\n",
    "    return {\"symbol\": symbol, \"results\": results}\n",
    "\n",
    "# In kết quả\n",
    "for sym in symbols:\n",
    "    result = train_all_models_for_one_symbol(sym)\n",
    "    if result:\n",
    "        for model, metrics in result[\"results\"].items():\n",
    "            print(f\"Model: {model}, Symbol: {result['symbol']}, \"\n",
    "                  f\"RMSE: {metrics['rmse']}, MSE: {metrics['mse']}, \"\n",
    "                  f\"MAPE: {metrics['mape']}, R2: {metrics['r2']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_close(symbol: str):\n",
    "    print(f\"\\n🔮 Predicting next CLOSE for: {symbol}\")\n",
    "    df = load_data_from_postgres(symbol)\n",
    "    print(df.head())\n",
    "    print(f\"📊 Loaded data for {symbol}: {len(df)} rows\")\n",
    "    if df is None or len(df) < 8:\n",
    "        print(f\"⚠️ Not enough data for prediction: {symbol}\")\n",
    "        return None\n",
    "\n",
    "    df = df.sort_values(\"time\", ascending=False).head(10000).sort_values(\"time\")\n",
    "    df = remove_outliers_iqr(df, [\"close\"])\n",
    "\n",
    "    # Dựa vào các mô hình đã train trước đó, đưa ra dự đoán cho giá CLOSE tiếp theo bằng cả 3 mô hình\n",
    "    # 1. Linear Regression không cần fit lại\n",
    "    linear = train_linear_regression(df, df)\n",
    "    linear_model = LinearRegression()\n",
    "    linear_model.fit(df.drop(columns=[\"NEXT_CLOSE\"]).values, df[\"NEXT_CLOSE\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f1fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_next_close(\"BTCUSDT\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training models for: BTCUSDT with anti-overfitting techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hoain\\AppData\\Local\\Temp\\ipykernel_6588\\59782643.py:31: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📉 Removed 0 outliers from 'close'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\hoain\\\\AppData\\\\Local\\\\Temp\\\\spark-51e85a69-8968-4163-a5fc-a687b3b91632\\\\pyspark-4cf919fa-428c-4088-9e01-3cc48d0bfd87\\\\tmpk30vednf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 372\u001b[39m\n\u001b[32m    370\u001b[39m symbols = [\u001b[33m\"\u001b[39m\u001b[33mBTCUSDT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mETHUSDT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBNBUSDT\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m symbols:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     results = \u001b[43mtrain_all_models_with_anti_overfitting\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# Save models for later use\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 332\u001b[39m, in \u001b[36mtrain_all_models_with_anti_overfitting\u001b[39m\u001b[34m(symbol)\u001b[39m\n\u001b[32m    329\u001b[39m         df[col_name] = df[col_name] * scale_factor\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Create Spark DataFrame\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m spark_df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Use time series cross-validation\u001b[39;00m\n\u001b[32m    335\u001b[39m cv_results = time_series_cross_validation(\n\u001b[32m    336\u001b[39m     spark_df, \n\u001b[32m    337\u001b[39m     n_splits=\u001b[32m5\u001b[39m,\n\u001b[32m    338\u001b[39m     prediction_window=\u001b[33m\"\u001b[39m\u001b[33m1 hour\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Use daily predictions instead of hourly\u001b[39;00m\n\u001b[32m    339\u001b[39m     lag_periods=\u001b[32m7\u001b[39m               \u001b[38;5;66;03m# Use fewer lag periods\u001b[39;00m\n\u001b[32m    340\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1440\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1436\u001b[39m     data = pd.DataFrame(data, columns=column_names)\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[32m   1441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_dataframe(\n\u001b[32m   1444\u001b[39m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1445\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:363\u001b[39m, in \u001b[36mSparkConversionMixin.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    361\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    362\u001b[39m converted_data = \u001b[38;5;28mself\u001b[39m._convert_from_pandas(data, schema, timezone)\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1483\u001b[39m     rdd, struct = \u001b[38;5;28mself\u001b[39m._createFromRDD(data.map(prepare), schema, samplingRatio)\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m     rdd, struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\session.py:1116\u001b[39m, in \u001b[36mSparkSession._createFromLocal\u001b[39m\u001b[34m(self, data, schema)\u001b[39m\n\u001b[32m   1114\u001b[39m \u001b[38;5;66;03m# convert python objects to sql data\u001b[39;00m\n\u001b[32m   1115\u001b[39m internal_data = [struct.toInternal(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tupled_data]\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minternal_data\u001b[49m\u001b[43m)\u001b[49m, struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:824\u001b[39m, in \u001b[36mSparkContext.parallelize\u001b[39m\u001b[34m(self, c, numSlices)\u001b[39m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    822\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm.PythonParallelizeServer(\u001b[38;5;28mself\u001b[39m._jsc.sc(), numSlices)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m jrdd = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialize_to_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreateRDDServer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(jrdd, \u001b[38;5;28mself\u001b[39m, serializer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\context.py:864\u001b[39m, in \u001b[36mSparkContext._serialize_to_jvm\u001b[39m\u001b[34m(self, data, serializer, reader_func, server_func)\u001b[39m\n\u001b[32m    860\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# without encryption, we serialize to a file, and we read the file in java and\u001b[39;00m\n\u001b[32m    863\u001b[39m     \u001b[38;5;66;03m# parallelize from there.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m     tempFile = \u001b[43mNamedTemporaryFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelete\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_temp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    866\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py:563\u001b[39m, in \u001b[36mNamedTemporaryFile\u001b[39m\u001b[34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     file = _io.open(\u001b[38;5;28mdir\u001b[39m, mode, buffering=buffering,\n\u001b[32m    564\u001b[39m                     newline=newline, encoding=encoding, errors=errors,\n\u001b[32m    565\u001b[39m                     opener=opener)\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    567\u001b[39m         raw = \u001b[38;5;28mgetattr\u001b[39m(file, \u001b[33m'\u001b[39m\u001b[33mbuffer\u001b[39m\u001b[33m'\u001b[39m, file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py:560\u001b[39m, in \u001b[36mNamedTemporaryFile.<locals>.opener\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mopener\u001b[39m(*args):\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mnonlocal\u001b[39;00m name\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     fd, name = \u001b[43m_mkstemp_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fd\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoain\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tempfile.py:256\u001b[39m, in \u001b[36m_mkstemp_inner\u001b[39m\u001b[34m(dir, pre, suf, flags, output_type)\u001b[39m\n\u001b[32m    254\u001b[39m _sys.audit(\u001b[33m\"\u001b[39m\u001b[33mtempfile.mkstemp\u001b[39m\u001b[33m\"\u001b[39m, file)\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     fd = _os.open(file, flags, \u001b[32m0o600\u001b[39m)\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m    \u001b[38;5;66;03m# try again\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\hoain\\\\AppData\\\\Local\\\\Temp\\\\spark-51e85a69-8968-4163-a5fc-a687b3b91632\\\\pyspark-4cf919fa-428c-4088-9e01-3cc48d0bfd87\\\\tmpk30vednf'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd31cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Libraries\n",
    "# import numpy as np  # to handle matrix\n",
    "# import pandas as pd # to handle data\n",
    "# from matplotlib import pyplot as plt # to visualize\n",
    "# import datetime, pytz # to handle time\n",
    "# from sklearn.model_selection import train_test_split # Split data\n",
    "# from sklearn.ensemble import RandomForestRegressor # Random Forest Classifier\n",
    "# import logging\n",
    "# import psycopg2\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# import psycopg2\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, lag, avg\n",
    "# from pyspark.sql.window import Window\n",
    "# from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "# from pyspark.ml.regression import GBTRegressor\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.sql.functions import abs as sql_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def connect_to_postgres():\n",
    "#     try:\n",
    "#         conn = psycopg2.connect(\n",
    "#             dbname=\"airflow\",\n",
    "#             user=\"airflow\",\n",
    "#             password=\"airflow\",\n",
    "#             host=\"localhost\",\n",
    "#             port=\"5432\"\n",
    "#         )\n",
    "#         logging.info(\"✅ Connected to PostgreSQL.\")\n",
    "#         return conn\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"❌ Failed to connect to PostgreSQL: {e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77133202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo Spark session và connect to PostgreSQL bằng cách chạy hàm connect_to_postgres\n",
    "# spark = SparkSession.builder.master(\"local\").appName(\"Crypto\").getOrCreate()\n",
    "# conn = connect_to_postgres()\n",
    "# # Nếu kết nối thành công, in ra dataframe từ bảng crypto_prices\n",
    "# if conn:\n",
    "#     # Read data from PostgreSQL\n",
    "#     query = \"SELECT * FROM crypto_data WHERE symbol = 'BNBUSDT';\"\n",
    "#     df = pd.read_sql(query, conn)\n",
    "#     # Convert the DataFrame to a Spark DataFrame\n",
    "#     spark_df = spark.createDataFrame(df)\n",
    "#     # Close the connection\n",
    "#     conn.close()\n",
    "#     # In ra 5 dòng đầu tiên của dataframe\n",
    "#     spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In ra shape của dataframe\n",
    "# print(f\"Shape of DataFrame: {spark_df.count()} rows, {len(spark_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, window, first\n",
    "# import datetime\n",
    "\n",
    "# start = datetime.datetime(2023, 1, 1, 0, 0, 0)\n",
    "# end = datetime.datetime(2025, 10, 1, 0, 0, 0)\n",
    "\n",
    "# # Filter theo khoảng thời gian\n",
    "# spark_df = spark_df.filter((col(\"time\") >= start) & (col(\"time\") <= end))\n",
    "\n",
    "# # Group by theo cửa sổ thời gian 1 giờ\n",
    "# spark_df = spark_df.groupBy(\n",
    "#     window(col(\"time\"), \"3 hours\")\n",
    "# ).agg(\n",
    "#     *[first(col_name).alias(col_name) for col_name in spark_df.columns if col_name != \"time\"]\n",
    "# )\n",
    "\n",
    "# # Nếu muốn hiển thị thời gian bắt đầu mốc group\n",
    "# spark_df = spark_df.withColumn(\"time_group\", col(\"window.start\")).drop(\"window\")\n",
    "\n",
    "# # Hiển thị 3 dòng đầu\n",
    "# spark_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa241202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lag\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # Xác định cửa sổ để tính toán lag\n",
    "# window_spec = Window.orderBy(\"time_group\")\n",
    "\n",
    "# # Thêm các cột lịch sử (lag features) cho 7 ngày\n",
    "# for i in range(1, 8):  # for 7 days\n",
    "#     spark_df = spark_df.withColumn(f\"open_b_{i}\", lag(\"open\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"high_b_{i}\", lag(\"high\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"low_b_{i}\", lag(\"low\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"close_b_{i}\", lag(\"close\", i).over(window_spec))\n",
    "#     spark_df = spark_df.withColumn(f\"volume_b_{i}\", lag(\"volume\", i).over(window_spec))\n",
    "\n",
    "# # Loại bỏ các hàng không đủ dữ liệu lịch sử\n",
    "# spark_df = spark_df.dropna()\n",
    "\n",
    "# # Hiển thị thông tin về dữ liệu sau khi thêm cột lịch sử\n",
    "# print(f\"Historical Data Shape: {spark_df.count()} rows, {len(spark_df.columns)} columns\")\n",
    "# spark_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f935b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lead\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # Xác định cửa sổ để tính toán giá trị tiếp theo\n",
    "# window_spec = Window.orderBy(\"time_group\")\n",
    "\n",
    "# # Thêm cột \"NEXT_CLOSE\" làm nhãn (label) cho giá trị đóng cửa tiếp theo\n",
    "# spark_df = spark_df.withColumn(\"NEXT_CLOSE\", lead(\"close\", 1).over(window_spec))\n",
    "\n",
    "# # Loại bỏ các hàng không đủ dữ liệu (hàng cuối cùng không có giá trị tiếp theo)\n",
    "# spark_df = spark_df.dropna()\n",
    "\n",
    "# # Loại bỏ cột symbol và time vì không cần thiết cho mô hình\n",
    "# spark_df = spark_df.drop(\"symbol\", \"time\", \"time_group\")\n",
    "\n",
    "\n",
    "# # Hiển thị thông tin về dữ liệu sau khi thêm cột nhãn\n",
    "# print(f\"After adding NEXT_CLOSE Label, new shape: {spark_df.count()} rows, {len(spark_df.columns)} columns\")\n",
    "# spark_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Số ngày dự đoán\n",
    "# prediction_days = 250\n",
    "# # Tổng số hàng trong DataFrame\n",
    "# total_rows = spark_df.count()\n",
    "\n",
    "# # Số hàng cho tập huấn luyện\n",
    "# train_rows = total_rows - prediction_days\n",
    "\n",
    "# # Chia dữ liệu thành tập train và test\n",
    "# train_df = spark_df.limit(train_rows)  # Lấy `train_rows` đầu tiên làm tập huấn luyện\n",
    "# test_df = spark_df.subtract(train_df)  # Phần còn lại là tập kiểm tra\n",
    "\n",
    "# # Hiển thị thông tin về tập train/test\n",
    "# print(f\"PERCENT test/total data = {(prediction_days / total_rows) * 100:.2f}%\")\n",
    "# print(f\"Train data shape: {train_df.count()} rows, {len(train_df.columns)} columns\")\n",
    "# print(f\"Test data shape: {test_df.count()} rows, {len(test_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_TrainingData = train_df\n",
    "# spark_TestData = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.regression import LabeledPoint\n",
    "# from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# transformed_TrainingData = spark_TrainingData.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n",
    "# transformed_TestData = spark_TestData.rdd.map(lambda row: LabeledPoint(row[-1], Vectors.dense(row[0:-1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of training set rows: %d\" % transformed_TrainingData.count())\n",
    "# print(\"Number of test set rows: %d\" % transformed_TestData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad97ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's make sure we have the correct types.\n",
    "# print(\"%s should be an RDD\" % type(transformed_TrainingData))\n",
    "# print(\"%s should be a LabeledPoint\" % type(transformed_TestData.first()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.tree import RandomForest\n",
    "# from time import *\n",
    "# RF_NUM_TREES = 1000 # 5\n",
    "# RF_MAX_DEPTH = 7 # 5\n",
    "# IMPURITY =\"variance\"\n",
    "# RF_MAX_BINS = 300\n",
    "# RANDOM_SEED = 13579 #None\n",
    "\n",
    "# start_time = time()\n",
    "\n",
    "# model = RandomForest.trainRegressor(transformed_TrainingData, categoricalFeaturesInfo={}, \\\n",
    "#     numTrees=RF_NUM_TREES, featureSubsetStrategy=\"auto\", impurity= IMPURITY, \\\n",
    "#     maxDepth=RF_MAX_DEPTH, maxBins=RF_MAX_BINS, seed=RANDOM_SEED)\n",
    "\n",
    "# end_time = time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions\n",
    "# predictions = model.predict(transformed_TestData.map(lambda x: x.features))\n",
    "\n",
    "# # Repartition both RDDs to ensure they have the same number of partitions\n",
    "# labels = transformed_TestData.map(lambda x: x.label).repartition(1)\n",
    "# predictions = predictions.repartition(1)\n",
    "\n",
    "# # Zip the RDDs after ensuring they are aligned\n",
    "# labels_and_predictions = labels.zip(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute accuracy\n",
    "# from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# start_time = time()\n",
    "\n",
    "# metrics = RegressionMetrics(labels_and_predictions)\n",
    "# print(\"Root Mean Squared Error: %.f\" % (metrics.rootMeanSquaredError))\n",
    "# print(\"R^2 Error: %.3f\" % (metrics.r2))\n",
    "\n",
    "# end_time = time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(\"Time to evaluate model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_Real = transformed_TestData.map(lambda x: x.label).collect()\n",
    "# list_Predicted = predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb94a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualising the results\n",
    "# plt.figure(figsize=(25,15), dpi=80, facecolor='w', edgecolor='k')\n",
    "# ax = plt.gca()  \n",
    "# # Convert test_df to Pandas DataFrame to access the index\n",
    "# test_df_pd = test_df.toPandas()\n",
    "# plt.plot(test_df_pd.index, list_Real, color='red', label='Real BTC Price')\n",
    "# plt.plot(test_df_pd.index, list_Predicted, color='blue', label='Predicted BTC Price')\n",
    "# plt.title('BTC Price Prediction', fontsize=40)\n",
    "# x = test_df_pd.reset_index().index\n",
    "# for tick in ax.xaxis.get_major_ticks():\n",
    "#     tick.label1.set_fontsize(18)\n",
    "# for tick in ax.yaxis.get_major_ticks():\n",
    "#     tick.label1.set_fontsize(18)\n",
    "# plt.xlabel('Time', fontsize=40)\n",
    "# plt.ylabel('BTC Price(USD) [Closed]', fontsize=40)\n",
    "# plt.legend(loc=2, prop={'size': 25})\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
